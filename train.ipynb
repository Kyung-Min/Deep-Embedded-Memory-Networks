{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Embedded Memory Networks (DEMN)\n",
    "##### Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang (Seoul National University & Surromind Robotics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how the DEMN works. The DEMN consists of three modules (video story understanding, story selection, answer selection). This code corresponds to QA modules (story selection, answer selection) among them. The results of the video understanding module are reflected in the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import utils\n",
    "\n",
    "import keras.activations as activations\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda, Permute, RepeatVector\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def config():\n",
    "    c = dict()\n",
    "    # embedding params\n",
    "    c['emb'] = 'Glove'\n",
    "    c['embdim'] = 300\n",
    "    c['inp_e_dropout'] = 1/2\n",
    "\n",
    "    # objective function\n",
    "    c['loss'] = 'ranking_loss'  \n",
    "    c['margin'] = 1\n",
    "\n",
    "    # training hyperparams\n",
    "    c['opt'] = 'adam'\n",
    "    c['batch_size'] = 160   \n",
    "    c['epochs'] = 16\n",
    "    \n",
    "    # sentences with word lengths below the 'pad' will be padded with 0.\n",
    "    c['pad'] = 60\n",
    "    \n",
    "    # scoring function: word-level attention-based model\n",
    "    c['dropout'] = 1/2     \n",
    "    c['dropoutfix_inp'] = 0\n",
    "    c['dropoutfix_rec'] = 0           \n",
    "    c['l2reg'] = 1e-4\n",
    "                                              \n",
    "    c['rnnbidi'] = True                      \n",
    "    c['rnn'] = GRU                                                     \n",
    "    c['rnnbidi_mode'] = add\n",
    "    c['rnnact'] = 'tanh'\n",
    "    c['rnninit'] = 'glorot_uniform'                      \n",
    "    c['sdim'] = 1\n",
    "\n",
    "    c['pool_layer'] = MaxPooling1D\n",
    "    c['cnnact'] = 'tanh'\n",
    "    c['cnninit'] = 'glorot_uniform'\n",
    "    c['cdim'] = 2\n",
    "    c['cfiltlen'] = 3\n",
    "    \n",
    "    c['adim'] = 1/2\n",
    "\n",
    "    # mlp scoring function\n",
    "    c['Ddim'] = 2\n",
    "    \n",
    "    ps, h = utils.hash_params(c)\n",
    "\n",
    "    return c, ps, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = None\n",
    "emb = None\n",
    "vocab = None\n",
    "inp_tr = None\n",
    "inp_val = None\n",
    "inp_test = None\n",
    "y_val = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load\n",
    "\n",
    "\n",
    "The data provided contain the output from the video story understanding module, i.e. reconstructed story sentence $s_i$, where \n",
    "\n",
    "$$ s_i = ê_i || l_i $$  \n",
    "- $ê_i$ is the description for the i-th video scene, which is retrieved by the video story understanding module <br>\n",
    "- $l_i$ is the subtitle of the i-th video scene <br>\n",
    "- || is concatenation <br>\n",
    "\n",
    "For example, $s_i$ can be ‘there are three friends on the ground. the friends are talking about the new house.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The format of the dataset is as follows.\n",
    "\n",
    "Training dataset:\n",
    "question1, positive story sentence, negative story sentence1, positive answer sentence, negative answer sentence1\n",
    "                                        ...                                           , negative answer sentence2\n",
    "                                        ...                                           , negative answer sentence3\n",
    "                                        ...                                           , negative answer sentence4\n",
    "question1, positive story sentence, negative story sentence2, positive answer sentence, negative answer sentence1\n",
    "                                        ...                                           , negative answer sentence2\n",
    "                                        ...                                           , negative answer sentence3\n",
    "                                        ...                                           , negative answer sentence4\n",
    "                                         \n",
    "                                         ...\n",
    "                                         \n",
    "question2, positive story sentence, negative story sentence1, positive answer sentence, negative answer sentence1\n",
    "                                        ...                                           , negative answer sentence2\n",
    "                                        ...                                           , negative answer sentence3\n",
    "                                        ...                                           , negative answer sentence4\n",
    "question2, positive story sentence, negative story sentence2, positive answer sentence, negative answer sentence1\n",
    "                                        ...                                           , negative answer sentence2\n",
    "                                        ...                                           , negative answer sentence3\n",
    "                                        ...                                           , negative answer sentence4\n",
    "\n",
    "Validation & test dataset:\n",
    "question1, label for story sentence, story sentence, dummy, positive answer sentence, negative answer sentence1\n",
    "                                      ...                                           , negative answer sentence2\n",
    "                                      ...                                           , negative answer sentence3\n",
    "                                      ...                                           , negative answer sentence4\n",
    "question1, label for story sentence, story sentence, dummy, positive answer sentence, negative answer sentence1\n",
    "                                      ...                                           , negative answer sentence2\n",
    "                                      ...                                           , negative answer sentence3\n",
    "                                      ...                                           , negative answer sentence4\n",
    "                                         \n",
    "                                      ...\n",
    "                                         \n",
    "question2, label for story sentence, story sentence, dummy, positive answer sentence, negative answer sentence1\n",
    "                                      ...                                           , negative answer sentence2\n",
    "                                      ...                                           , negative answer sentence3\n",
    "                                      ...                                           , negative answer sentence4\n",
    "question2, label for story sentence, story sentence, dummy, positive answer sentence, negative answer sentence1\n",
    "                                      ...                                           , negative answer sentence2\n",
    "                                      ...                                           , negative answer sentence3\n",
    "                                      ...                                           , negative answer sentence4\n",
    "'''\n",
    "\n",
    "def load_data_from_file(dsfile, iseval):\n",
    "    #load a dataset in the csv format;\n",
    "\n",
    "    q = [] # a set of questions\n",
    "    s_p = [] # if training time, s1 is a set of positive sentences. Otherwise, s1 is a set of sentences.\n",
    "    s_n = [] # if training time, s2 is a set of negative sentences. Otherwise, s2 is a set of dummy sentences.\n",
    "    q_sp = [] # a set of sentences which concatenate questions and positive sentences\n",
    "    a_p = [] # a set of positive answers\n",
    "    a_n = [] # a set of negative answers\n",
    "    labels = []\n",
    "\n",
    "    with open(dsfile) as f:\n",
    "        c = csv.DictReader(f)\n",
    "        for l in c:\n",
    "            if iseval:\n",
    "                label = int(l['label'])\n",
    "                labels.append(label)\n",
    "            try:\n",
    "                qtext = l['qtext'].decode('utf8')\n",
    "                s_p_text = l['atext1'].decode('utf8')\n",
    "                s_n_text = l['atext2'].decode('utf8')\n",
    "            except AttributeError:  # python3 has no .decode()\n",
    "                qtext = l['qtext']\n",
    "                s_p_text = l['atext1']\n",
    "                s_n_text = l['atext2']\n",
    "            a_p_text = l['a1'].decode('utf8')\n",
    "            a_n_text = l['a2'].decode('utf8')\n",
    "            a_p.append(a_p_text.split(' '))\n",
    "            a_n.append(a_n_text.split(' '))\n",
    "            \n",
    "            q.append(qtext.split(' '))\n",
    "            s_p.append(s_p_text.split(' '))\n",
    "            s_n.append(s_n_text.split(' '))\n",
    "            q_sp.append(qtext.split(' ')+s_p_text.split(' '))\n",
    "    if iseval:\n",
    "        return (q, s_p, s_n, q_sp, a_p, a_n, np.array(labels))\n",
    "    else:\n",
    "        return (q, s_p, s_n, q_sp, a_p, a_n)\n",
    "    \n",
    "def make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, f31, f13, f32, f23, \n",
    "                      q, s_p, s_n, q_sp, a_p, a_n, y=None):\n",
    "    inp = {'qi': qi, 'si_p': si_p, 'si_n': si_n, 'qi_si':qi_si, 'ai_p':ai_p, \n",
    "          'ai_n':ai_n, 'f01':f01, 'f10':f10, 'f02':f02, 'f20':f20, 'f31':f31, \n",
    "          'f13':f13, 'f32':f32, 'f23':f23, 'q':q, 's_p':s_n, 's_n':s_n, 'q_sp':q_sp, 'a_p':a_p, 'a_n':a_n} \n",
    "    \n",
    "    if y is not None:\n",
    "        inp['y'] = y\n",
    "    return inp\n",
    "\n",
    "def load_set(fname, vocab=None, iseval=False):\n",
    "    if iseval:\n",
    "        q, s_p, s_n, q_sp, a_p, a_n, y = load_data_from_file(fname, iseval)\n",
    "    else:\n",
    "        q, s_p, s_n, q_sp, a_p, a_n = load_data_from_file(fname, iseval)\n",
    "        vocab = utils.Vocabulary(q + s_p + s_n + a_p + a_n) \n",
    "    \n",
    "    pad = conf['pad']\n",
    "    \n",
    "    qi = vocab.vectorize(q, pad=pad)  \n",
    "    si_p = vocab.vectorize(s_p, pad=pad)\n",
    "    si_n = vocab.vectorize(s_n, pad=pad)\n",
    "    qi_si = vocab.vectorize(q_sp, pad=pad)\n",
    "    ai_p = vocab.vectorize(a_p, pad=pad)\n",
    "    ai_n = vocab.vectorize(a_n, pad=pad)\n",
    "    \n",
    "    f01, f10 = utils.sentence_flags(q, s_p, pad)  \n",
    "    f02, f20 = utils.sentence_flags(q, s_n, pad)\n",
    "    f31, f13 = utils.sentence_flags(q_sp, a_p, pad)\n",
    "    f32, f23 = utils.sentence_flags(q_sp, a_n, pad)\n",
    "    if iseval:\n",
    "        inp = make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, \n",
    "                                f31, f13, f32, f23, q, s_p, s_n, q_sp, a_p, a_n, y=y)\n",
    "        return (inp, y)\n",
    "    else:\n",
    "        inp = make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, \n",
    "                            f31, f13, f32, f23, q, s_p, s_n, q_sp, a_p, a_n)\n",
    "        return (inp, vocab)        \n",
    "    \n",
    "def load_data(trainf, valf, testf):\n",
    "    global vocab, inp_tr, inp_val, inp_test, y_val, y_test\n",
    "    inp_tr, vocab = load_set(trainf, iseval=False)\n",
    "    inp_val, y_val = load_set(valf, vocab=vocab, iseval=True)\n",
    "    inp_test, y_test = load_set(testf, vocab=vocab, iseval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding():\n",
    "    '''\n",
    "    Declare all inputs (vectorized sentences and NLP flags)\n",
    "    and generate outputs representing vector sequences with dropout applied.  \n",
    "    Returns the vector dimensionality.       \n",
    "    '''\n",
    "    pad = conf['pad']\n",
    "    dropout = conf['inp_e_dropout']\n",
    "    \n",
    "    # story selection\n",
    "    input_qi = Input(name='qi', shape=(pad,), dtype='int32')                          \n",
    "    input_si_p = Input(name='si_p', shape=(pad,), dtype='int32')                 \n",
    "    input_f01 = Input(name='f01', shape=(pad, utils.flagsdim))\n",
    "    input_f10 = Input(name='f10', shape=(pad, utils.flagsdim))\n",
    "\n",
    "    input_si_n = Input(name='si_n', shape=(pad,), dtype='int32')  \n",
    "    input_f02 = Input(name='f02', shape=(pad, utils.flagsdim))\n",
    "    input_f20 = Input(name='f20', shape=(pad, utils.flagsdim))             \n",
    "\n",
    "    # answer selection\n",
    "    input_qi_si = Input(name='qi_si', shape=(pad,), dtype='int32')\n",
    "    input_ai_p = Input(name='ai_p', shape=(pad,), dtype='int32')                        \n",
    "    input_f31 = Input(name='f31', shape=(pad, utils.flagsdim))              \n",
    "    input_f13 = Input(name='f13', shape=(pad, utils.flagsdim))          \n",
    "\n",
    "    input_ai_n = Input(name='ai_n', shape=(pad,), dtype='int32')         \n",
    "    input_f32 = Input(name='f32', shape=(pad, utils.flagsdim))            \n",
    "    input_f23 = Input(name='f23', shape=(pad, utils.flagsdim))                       \n",
    "\n",
    "    input_nodes = [input_qi, input_si_p, input_f01, input_f10, input_si_n,         \n",
    "            input_f02, input_f20, input_qi_si, input_ai_p, input_f31, input_f13,\n",
    "            input_ai_n, input_f32, input_f23]           \n",
    "        \n",
    "    N = emb.N + utils.flagsdim\n",
    "    shared_embedding = Embedding(name='emb', input_dim=vocab.size(), input_length=pad,\n",
    "                                output_dim=emb.N, mask_zero=False,\n",
    "                                weights=[vocab.embmatrix(emb)], trainable=True)\n",
    "    emb_qi_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi),\n",
    "        input_f01]))\n",
    "    emb_si_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_si_p),\n",
    "        input_f10]))\n",
    "    emb_qi_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi),\n",
    "        input_f02]))\n",
    "    emb_si_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_si_n),\n",
    "        input_f20]))\n",
    "    emb_qi_si_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi_si),\n",
    "        input_f31]))\n",
    "    emb_ai_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_ai_p),\n",
    "        input_f13]))\n",
    "    emb_qi_si_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi_si),\n",
    "        input_f32]))\n",
    "    emb_ai_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_ai_n),\n",
    "        input_f23]))\n",
    "\n",
    "    emb_outputs = [emb_qi_p, emb_si_p, emb_qi_n, emb_si_n, emb_qi_si_p, emb_ai_p, emb_qi_si_n, emb_ai_n]\n",
    "    \n",
    "    return N, input_nodes, emb_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Function\n",
    "\n",
    "To handle the long sentences, the word level attention-based model is used as the scoring functions G and H.\n",
    "\n",
    "The model builds the embeddings of two sequences of tokens X, Y. The model encodes each token of X, Y using a bidirectional LSTM and calculates the sentence vector X by applying a convolution on the output token vectors of the bidirectional LSTM on the X side. Then the each token vector of Y are multiplied by a softmax weight, which is determined by X. \n",
    "\n",
    "$$m(t)=tanh(W_ah_y(t)+W_qX)$$\n",
    "$$o_t \\propto exp(w^t_{ms}m(t))$$\n",
    "$$h^\\prime_y(t)=h_y(t)o_t$$\n",
    "\n",
    "where \n",
    "- $h_y(t)$ is the t-th token vector on the Y side.\n",
    "- $h^\\prime_y(t)$ is the\n",
    "updated t-th token vector. \n",
    "- $W_a, W_q, w_{ms}$ are attention\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_model(input_nodes, N, pfx=''):\n",
    "    # apply biLSTM on each sentence X,Y\n",
    "    qpos, pos, qneg, neg = rnn_input(N, pfx=pfx, dropout=conf['dropout'], dropoutfix_inp=conf['dropoutfix_inp'], \n",
    "                            dropoutfix_rec=conf['dropoutfix_rec'], sdim=conf['sdim'], \n",
    "                            rnnbidi_mode=conf['rnnbidi_mode'], rnn=conf['rnn'], rnnact=conf['rnnact'], \n",
    "                            rnninit=conf['rnninit'], inputs=input_nodes)\n",
    "    \n",
    "    # calculate the sentence vector on X side using Convolutional Neural Networks\n",
    "    qpos_aggreg, qneg_aggreg, gwidth = aggregate(qpos, qneg, 'aggre_q'+pfx, N, \n",
    "                                               dropout=conf['dropout'], l2reg=conf['l2reg'], \n",
    "                                               sdim=conf['sdim'], cnnact=conf['cnnact'], cdim=conf['cdim'], \n",
    "                                               cfiltlen=conf['cfiltlen'])\n",
    "    \n",
    "    # re-embed X,Y in attention space\n",
    "    awidth = int(N*conf['adim'])\n",
    "    \n",
    "    shared_dense_q = Dense(awidth, name='attn_proj_q'+pfx, kernel_regularizer=l2(conf['l2reg']))\n",
    "    qpos_aggreg_attn = shared_dense_q(qpos_aggreg)\n",
    "    qneg_aggreg_attn = shared_dense_q(qneg_aggreg)\n",
    "    \n",
    "    shared_dense_s = Dense(awidth, name='attn_proj_s'+pfx, kernel_regularizer=l2(conf['l2reg']))\n",
    "    pos_attn = TimeDistributed(shared_dense_s)(pos)\n",
    "    neg_attn = TimeDistributed(shared_dense_s)(neg)\n",
    "    \n",
    "    # apply an attention function on Y side by producing an vector of scalars denoting the attention for each token\n",
    "    pos_foc, neg_foc = focus(N, qpos_aggreg_attn, qneg_aggreg_attn, pos_attn, neg_attn, \n",
    "                             pos, neg, conf['sdim'], awidth, \n",
    "                             conf['l2reg'], pfx=pfx)\n",
    "\n",
    "    # calculate the sentence vector on Y side using Convolutional Neural Networks\n",
    "    pos_aggreg, neg_aggreg, gwidth = aggregate(pos_foc, neg_foc, 'aggre_s'+pfx, N, \n",
    "                                  dropout=conf['dropout'], l2reg=conf['l2reg'], sdim=conf['sdim'],\n",
    "                                  cnnact=conf['cnnact'], cdim=conf['cdim'], cfiltlen=conf['cfiltlen'])\n",
    "\n",
    "    return ([qpos_aggreg, pos_aggreg], [qneg_aggreg, neg_aggreg]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_input(N, dropout=3/4, dropoutfix_inp=0, dropoutfix_rec=0,           \n",
    "              sdim=2, rnn=GRU, rnnact='tanh', rnninit='glorot_uniform', rnnbidi_mode=add, \n",
    "              inputs=None, pfx=''):\n",
    "    if rnnbidi_mode == 'concat':\n",
    "        sdim /= 2\n",
    "    shared_rnn_f = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N), \n",
    "                       activation=rnnact, return_sequences=True, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, name='rnnf'+pfx)\n",
    "    shared_rnn_b = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N),\n",
    "                       activation=rnnact, return_sequences=True, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, go_backwards=True, name='rnnb'+pfx)\n",
    "    qpos_f = shared_rnn_f(inputs[0])\n",
    "    pos_f = shared_rnn_f(inputs[1])\n",
    "    qneg_f = shared_rnn_f(inputs[2])\n",
    "    neg_f = shared_rnn_f(inputs[3])\n",
    "    \n",
    "    qpos_b = shared_rnn_b(inputs[0])\n",
    "    pos_b = shared_rnn_b(inputs[1])\n",
    "    qneg_b = shared_rnn_b(inputs[2])\n",
    "    neg_b = shared_rnn_b(inputs[3])\n",
    "\n",
    "    qpos = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([qpos_f, qpos_b]))\n",
    "    pos = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([pos_f, pos_b]))\n",
    "    qneg = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([qneg_f, qneg_b]))\n",
    "    neg = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([neg_f, neg_b]))\n",
    "    \n",
    "    return (qpos, pos, qneg, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate(in1, in2, pfx, N, dropout, l2reg, sdim, cnnact, cdim, cfiltlen):\n",
    "    '''\n",
    "    In the paper, the sentence vector was calculated using simple averagring, \n",
    "    but we will use Convolutional Neural Networks in the demo.\n",
    "    '''\n",
    "    \n",
    "    shared_conv = Convolution1D(name=pfx+'c', input_shape=(conf['pad'], int(N*sdim)), kernel_size=cfiltlen, \n",
    "                                filters=int(N*cdim), activation=cnnact, kernel_regularizer=l2(l2reg))\n",
    "    aggreg1 = shared_conv(in1)\n",
    "    aggreg2 = shared_conv(in2)\n",
    "\n",
    "    nsteps = conf['pad'] - cfiltlen + 1\n",
    "    width = int(N*cdim)\n",
    "    \n",
    "    aggreg1, aggreg2 = pool(pfx, aggreg1, aggreg2, nsteps, width, dropout=dropout)\n",
    "    \n",
    "    return (aggreg1, aggreg2, width)\n",
    "\n",
    "def pool(pfx, in1, in2, nsteps, width, dropout):\n",
    "    pooling = MaxPooling1D(pool_size=nsteps, name=pfx+'pool[0]')\n",
    "    out1 = pooling(in1)\n",
    "    out2 = pooling(in2)\n",
    "    \n",
    "    flatten = Flatten(name=pfx+'pool[1]')\n",
    "    out1 = Dropout(dropout, noise_shape=(1, width))(flatten(out1))\n",
    "    out2 = Dropout(dropout, noise_shape=(1, width))(flatten(out2))\n",
    "    \n",
    "    return (out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focus(N, input_aggreg1, input_aggreg2, input_seq1, input_seq2, orig_seq1, orig_seq2,\n",
    "          sdim, awidth, l2reg, pfx=''):\n",
    "    \n",
    "    repeat_vec = RepeatVector(conf['pad'], name='input_aggreg1_rep'+pfx)\n",
    "    input_aggreg1_rep = repeat_vec(input_aggreg1)\n",
    "    input_aggreg2_rep = repeat_vec(input_aggreg2)\n",
    "    \n",
    "    attn1 = Activation('tanh')(add([input_aggreg1_rep, input_seq1]))\n",
    "    attn2 = Activation('tanh')(add([input_aggreg2_rep, input_seq2]))\n",
    "    \n",
    "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), name='focus1'+pfx)\n",
    "    attn1 = TimeDistributed(shared_dense)(attn1)\n",
    "    attn2 = TimeDistributed(shared_dense)(attn2)\n",
    "    \n",
    "    flatten = Flatten(name='attn_flatten'+pfx)\n",
    "    attn1 = flatten(attn1)\n",
    "    attn2 = flatten(attn2)\n",
    "    \n",
    "    attn1 = Activation('softmax')(attn1)\n",
    "    attn1 = RepeatVector(int(N*sdim))(attn1)\n",
    "    attn1 = Permute((2,1))(attn1)\n",
    "    output1 = multiply([orig_seq1, attn1])\n",
    "    \n",
    "    attn2 = Activation('softmax')(attn2)\n",
    "    attn2 = RepeatVector(int(N*sdim))(attn2)\n",
    "    attn2 = Permute((2,1))(attn2)\n",
    "    output2 = multiply([orig_seq2, attn2])\n",
    "    \n",
    "    return (output1, output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare two sentence vectors, we used cosines similarity measure in the paper, but in the demo we use the mlp similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_ptscorer(inputs1, inputs2,  Ddim, N, l2reg, pfx='out', oact='sigmoid', extra_inp=[]):\n",
    "    \"\"\" Element-wise features from the pair fed to an MLP. \"\"\"\n",
    "\n",
    "    sum1 = add(inputs1)\n",
    "    sum2 = add(inputs2)\n",
    "    mul1 = multiply(inputs1)\n",
    "    mul2 = multiply(inputs2)\n",
    "\n",
    "    mlp_input1 = concatenate([sum1, mul1])\n",
    "    mlp_input2 = concatenate([sum2, mul2])\n",
    "\n",
    "    # Ddim may be either 0 (no hidden layer), scalar (single hidden layer) or\n",
    "    # list (multiple hidden layers)\n",
    "    if Ddim == 0:\n",
    "        Ddim = []\n",
    "    elif not isinstance(Ddim, list):\n",
    "        Ddim = [Ddim]\n",
    "    if Ddim:\n",
    "        for i, D in enumerate(Ddim):\n",
    "            shared_dense = Dense(int(N*D), kernel_regularizer=l2(l2reg), \n",
    "                                 activation='tanh', name=pfx+'hdn[%d]'%(i,))\n",
    "            mlp_input1 = shared_dense(mlp_input1)\n",
    "            mlp_input2 = shared_dense(mlp_input2)\n",
    "\n",
    "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), activation=oact, name=pfx+'mlp')\n",
    "    mlp_out1 = shared_dense(mlp_input1)\n",
    "    mlp_out2 = shared_dense(mlp_input2)\n",
    "    \n",
    "    return [mlp_out1, mlp_out2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_model(input_nodes, N, pfx=None):\n",
    "    shared_dense = Dense(int(N), activation='tanh', name='wproj'+pfx)\n",
    "    wproj1 = TimeDistributed(shared_dense)(input_nodes[0])\n",
    "    wproj2 = TimeDistributed(shared_dense)(input_nodes[1])\n",
    "    wproj3 = TimeDistributed(shared_dense)(input_nodes[2])\n",
    "    wproj4 = TimeDistributed(shared_dense)(input_nodes[3])\n",
    "    \n",
    "    avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
    "    e01b = avg_layer(wproj1)\n",
    "    e10b = avg_layer(wproj2)\n",
    "    e02b = avg_layer(wproj3)\n",
    "    e20b = avg_layer(wproj4)\n",
    "\n",
    "    shared_dense = Dense(name='deep'+pfx, output_dim=N, kernel_initializer='glorot_uniform', activation='tanh', W_regularizer=l2(conf['l2reg']))\n",
    "    e01b = shared_dense(e01b)\n",
    "    e10b = shared_dense(e10b)\n",
    "    e02b = shared_dense(e02b)\n",
    "    e20b = shared_dense(e20b)\n",
    "\n",
    "    shared_dense = Dense(name='proj'+pfx, input_dim=N, output_dim=int(N),\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation='tanh', kernel_regularizer=l2(conf['l2reg']))\n",
    "    e01p = shared_dense(e01b)\n",
    "    e10p = shared_dense(e10b)\n",
    "    e02p = shared_dense(e02b)\n",
    "    e20p = shared_dense(e20b)\n",
    "        \n",
    "\n",
    "    output_nodes1 = [e01p, e10p]\n",
    "    output_nodes2 = [e02p, e20p]\n",
    "\n",
    "    return (output_nodes1, output_nodes2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # input embedding         \n",
    "    N, input_nodes_emb, output_nodes_emb = embedding()\n",
    "    \n",
    "    # story selection\n",
    "    ptscorer_inputs1, ptscorer_inputs2 = avg_model(output_nodes_emb[:4], N, pfx='S')\n",
    "\n",
    "    scoreS1, scoreS2 = mlp_ptscorer(ptscorer_inputs1, ptscorer_inputs2, conf['Ddim'], N,  \n",
    "            conf['l2reg'], pfx='outS', oact='sigmoid')                \n",
    "\n",
    "    # anwer selection\n",
    "    ptscorer_inputs3, ptscorer_inputs4 = avg_model(output_nodes_emb[4:], N, pfx='A')\n",
    "    \n",
    "    scoreA1, scoreA2 = mlp_ptscorer(ptscorer_inputs3, ptscorer_inputs4, conf['Ddim'], N,\n",
    "            conf['l2reg'], pfx='outA', oact='sigmoid')\n",
    "\n",
    "    output_nodes = [scoreS1, scoreS2, scoreA1, scoreA2]\n",
    "\n",
    "    model = Model(inputs=input_nodes_emb, outputs=output_nodes)\n",
    "    \n",
    "    model.compile(loss=ranking_loss, optimizer=conf['opt'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Training is performed with a hinge rank loss over these two triplets:\n",
    "\n",
    "$$\\sum_{s_i \\neq s^*}^{|X|} max(0, \\gamma_s - G(q,s^*) + G(q,s_i)) + \\sum_{a_r \\neq a^*}^{k} max(0, \\gamma_a - H(s_a,s^*) + H(s_a, a_r))$$\n",
    "\n",
    "where \n",
    "- $s^*$ is the correct relevant story for q, i.e. $s^* = ê_c || l_c$\n",
    "- $a^*$ is the correct answer sentence for q. \n",
    "- $γ_s$ and $γ_a$ are margins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "posS: G(q, s^*)\n",
    "negS: G(q, s_i)\n",
    "posA: H(s_a, s^*)\n",
    "negA: H(s_a, a_r)\n",
    "'''\n",
    "def ranking_loss(y_true, y_pred):\n",
    "    posS = y_pred[0]\n",
    "    negS = y_pred[1]\n",
    "    posA = y_pred[2]\n",
    "    negA = y_pred[3]\n",
    "\n",
    "    margin = conf['margin']\n",
    "    loss = K.maximum(margin + negS - posS, 0.0) + K.maximum(margin + negA - posA, 0.0) \n",
    "    return K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(runid):\n",
    "    print('Model')\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "    \n",
    "    print('Training')\n",
    "    fit_model(model, weightsf='weights-'+runid+'-bestval.h5')\n",
    "    model.save_weights('weights-'+runid+'-final.h5', overwrite=True)\n",
    "    model.load_weights('weights-'+runid+'-bestval.h5')\n",
    "\n",
    "    print('Predict&Eval (best val epoch)')\n",
    "    res = eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, **kwargs):\n",
    "    epochs = conf['epochs']\n",
    "    callbacks = fit_callbacks(kwargs.pop('weightsf'))\n",
    "    \n",
    "    # During the computation, these values will not be used at all.\n",
    "    # Note that the variable 'y_true' in function ranking_loss does not participate in calculations.\n",
    "    dummy1 = np.ones((len(inp_tr['qi']),1), dtype=np.float) \n",
    "    dummy2 = np.ones((len(inp_val['qi']),1), dtype=np.float)\n",
    "    \n",
    "    return model.fit(inp_tr, y=[dummy1,dummy1,dummy1,dummy1], validation_data=[inp_val,\n",
    "        [dummy2,dummy2,dummy2,dummy2]], callbacks = callbacks, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At every epoch, the callback function measures mrr performance and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_callbacks(weightsf):                                  \n",
    "    return [utils.AnsSelCB(inp_val['q'], inp_val['s_p'], inp_val['s_n'], inp_val['q_sp'], \n",
    "        inp_val['a_p'], inp_val['a_n'], y_val, inp_val),\n",
    "            ModelCheckpoint(weightsf, save_best_only=True, monitor='acc', mode='max'),\n",
    "            EarlyStopping(monitor='acc', mode='max', patience=12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    res = []\n",
    "    for inp in [inp_val, inp_test]:\n",
    "        if inp is None:\n",
    "            res.append(None)\n",
    "            continue\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        ypredS = pred[0]\n",
    "        ypredA1 = pred[2]\n",
    "        ypredA2 = pred[3]\n",
    "\n",
    "        res.append(utils.eval_QA(ypredS, ypredA1, ypredA2, inp['q'], inp['y'], MAP=False))\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainf = 'data/anssel/pororo/train_triplet_concat_a5_500.csv' \n",
    "    valf = 'data/anssel/pororo/dev_triplet_concat_a5_for_mrr_500.csv'\n",
    "    testf = 'data/anssel/pororo/dev_triplet_concat_a5_for_mrr_500.csv'\n",
    "    params = []\n",
    "    \n",
    "    conf, ps, h = config()\n",
    "\n",
    "    if conf['emb'] == 'Glove':\n",
    "        print('GloVe')\n",
    "        emb = utils.GloVe(N=conf['embdim'])\n",
    "\n",
    "    print('Dataset')\n",
    "    load_data(trainf,valf,testf)\n",
    "    runid = 'DEMN-%x' % (h)\n",
    "    print('RunID: %s  (%s)' % (runid, ps))\n",
    "    train_and_eval(runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
